{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "14rpPt7TqFJTAv6cANIUbwFpdDWAbda6T",
      "authorship_tag": "ABX9TyManzZqou1Gd0lOQOuOsS/b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chengulatj/arrowhead-traffic-analysis/blob/main/Chiefs_Bill.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pykml\n",
        "!pip install fastkml\n",
        "!pip install polyline\n",
        "!pip install pytz haversine requests polyline pandas folium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mfljXi62pz4",
        "outputId": "9a4d214c-7325-41a2-877a-25ece1868c4d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pykml\n",
            "  Downloading pykml-0.2.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: lxml>=3.3.6 in /usr/local/lib/python3.11/dist-packages (from pykml) (5.3.0)\n",
            "Downloading pykml-0.2.0-py3-none-any.whl (41 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.1/41.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pykml\n",
            "Successfully installed pykml-0.2.0\n",
            "Collecting fastkml\n",
            "  Downloading fastkml-1.1.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting arrow (from fastkml)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pygeoif>=1.5 (from fastkml)\n",
            "  Downloading pygeoif-1.5.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: typing-extensions>4 in /usr/local/lib/python3.11/dist-packages (from fastkml) (4.12.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from arrow->fastkml) (2.8.2)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow->fastkml)\n",
            "  Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7.0->arrow->fastkml) (1.17.0)\n",
            "Downloading fastkml-1.1.0-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.9/107.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pygeoif-1.5.1-py3-none-any.whl (28 kB)\n",
            "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: types-python-dateutil, pygeoif, arrow, fastkml\n",
            "Successfully installed arrow-1.3.0 fastkml-1.1.0 pygeoif-1.5.1 types-python-dateutil-2.9.0.20241206\n",
            "Collecting polyline\n",
            "  Downloading polyline-2.0.2-py3-none-any.whl.metadata (6.4 kB)\n",
            "Downloading polyline-2.0.2-py3-none-any.whl (6.0 kB)\n",
            "Installing collected packages: polyline\n",
            "Successfully installed polyline-2.0.2\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (2024.2)\n",
            "Collecting haversine\n",
            "  Downloading haversine-2.9.0-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: polyline in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: folium in /usr/local/lib/python3.11/dist-packages (0.19.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: branca>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from folium) (0.8.1)\n",
            "Requirement already satisfied: jinja2>=2.9 in /usr/local/lib/python3.11/dist-packages (from folium) (3.1.5)\n",
            "Requirement already satisfied: xyzservices in /usr/local/lib/python3.11/dist-packages (from folium) (2025.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.9->folium) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading haversine-2.9.0-py2.py3-none-any.whl (7.7 kB)\n",
            "Installing collected packages: haversine\n",
            "Successfully installed haversine-2.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "import folium\n",
        "\n",
        "# Paths to your files\n",
        "kmz_cams_file = \"/content/drive/MyDrive/cams.kmz\"  # Path to your KMZ file\n",
        "\n",
        "# Extract the KML file from the KMZ file\n",
        "kml_extracted_path = \"/content/drive/MyDrive/extracted_kml.kml\"\n",
        "with ZipFile(kmz_cams_file, 'r') as kmz:\n",
        "    # Extract the KML file\n",
        "    kml_filename = [file for file in kmz.namelist() if file.endswith('.kml')][0]\n",
        "    kmz.extract(kml_filename, path=\"/content/drive/MyDrive\")\n",
        "    kml_full_path = f\"/content/drive/MyDrive/{kml_filename}\"\n",
        "\n",
        "# Parse the extracted KML file to retrieve placemark data\n",
        "tree = ET.parse(kml_full_path)\n",
        "root = tree.getroot()\n",
        "\n",
        "# Define the namespace to parse KML properly\n",
        "namespace = {\"kml\": \"http://www.opengis.net/kml/2.2\"}\n",
        "\n",
        "# Extract placemark names and coordinates\n",
        "camera_data = []\n",
        "for placemark in root.findall(\".//kml:Placemark\", namespace):\n",
        "    name = placemark.find(\"kml:name\", namespace).text\n",
        "    coords = placemark.find(\".//kml:coordinates\", namespace).text.strip()\n",
        "    lon, lat, _ = map(float, coords.split(\",\"))\n",
        "    camera_data.append({\"Name\": name, \"Latitude\": lat, \"Longitude\": lon})\n",
        "\n",
        "# Convert to a DataFrame\n",
        "camera_df = pd.DataFrame(camera_data)\n",
        "\n",
        "# Remove rows where 'Name' contains \"Arrowhead\" since that's the actual destination\n",
        "camera_df = camera_df[~camera_df[\"Name\"].str.contains(\"|\".join([\"Arrowhead\", \"North\", \"East\", \"West\", \"South\"]), case=False, na=False)]\n",
        "\n",
        "# Display the Camera DataFrame\n",
        "print(\"Camera  Locations:\")\n",
        "print(camera_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8b9lBLLDYOA",
        "outputId": "dec0d7eb-9c7e-4bbe-e440-bd90627779ed",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Camera  Locations:\n",
            "                              Name   Latitude  Longitude\n",
            "0   I-70 EB Past Blue Ridge Cutoff  39.047970 -94.466341\n",
            "1              I-70 EB @ US-40 Hwy  39.047025 -94.444162\n",
            "2       I-70 EB at Blue Ridge Blvd  39.046588 -94.437408\n",
            "3               I-70 EB at 31st St  39.067333 -94.509992\n",
            "4           I-70 WB Past Noland Rd  39.046587 -94.424266\n",
            "5           I-70 EB Past Noland Rd  39.045194 -94.399116\n",
            "6       I-70 EB at Lee's Summit Rd  39.044593 -94.386910\n",
            "8              I-435 SB at 23rd St  39.081620 -94.491118\n",
            "9      I-70 WB Past Van Brunt Blvd  39.072746 -94.526543\n",
            "10            I-70 EB Before I-470  39.043612 -94.377263\n",
            "11          I-435 SB at Raytown Rd  39.050540 -94.499744\n",
            "13          I-435 SB at Stadium Dr  39.055722 -94.494419\n",
            "14   I-70 EB at SE Corner of I-435  39.057050 -94.487158\n",
            "15           I-70 WB at Stadium Dr  39.055973 -94.481648\n",
            "16               I-435 NB at US_40  39.062850 -94.487709\n",
            "17      I-70 EB at Manchester Tfwy  39.063150 -94.500015\n",
            "18   I-70 EB at SW Corner of I-435  39.058884 -94.492161\n",
            "19                 I-70 EB at I435  39.060450 -94.494253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1H7MX5ieohPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to your files\n",
        "kmz_dms_file = \"/content/drive/MyDrive/dms.kmz\"  # Path to your KMZ file\n",
        "\n",
        "# Extract the KML file from the KMZ file\n",
        "kml_extracted_path = \"/content/drive/MyDrive/extracted_kml.kml\"\n",
        "with ZipFile(kmz_dms_file, 'r') as kmz:\n",
        "    # Extract the KML file\n",
        "    kml_filename = [file for file in kmz.namelist() if file.endswith('.kml')][0]\n",
        "    kmz.extract(kml_filename, path=\"/content/drive/MyDrive\")\n",
        "    kml_full_path = f\"/content/drive/MyDrive/{kml_filename}\"\n",
        "\n",
        "# Parse the extracted KML file to retrieve placemark data\n",
        "tree = ET.parse(kml_full_path)\n",
        "root = tree.getroot()\n",
        "\n",
        "# Define the namespace to parse KML properly\n",
        "namespace = {\"kml\": \"http://www.opengis.net/kml/2.2\"}\n",
        "\n",
        "# Extract placemark names and coordinates\n",
        "dms_data = []\n",
        "for placemark in root.findall(\".//kml:Placemark\", namespace):\n",
        "    name = placemark.find(\"kml:name\", namespace).text\n",
        "    coords = placemark.find(\".//kml:coordinates\", namespace).text.strip()\n",
        "    lon, lat, _ = map(float, coords.split(\",\"))\n",
        "    dms_data.append({\"Name\": name, \"Latitude\": lat, \"Longitude\": lon})\n",
        "\n",
        "# Convert to a DataFrame\n",
        "dms_df = pd.DataFrame(dms_data)\n",
        "\n",
        "# Display the Camera and DMS DataFrame\n",
        "print(\"DMS Locations:\")\n",
        "print(dms_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGutxeIaMjJB",
        "outputId": "4375769b-95d1-4100-9c3f-c55337cb8af3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DMS Locations:\n",
            "                              Name   Latitude  Longitude\n",
            "0        I-435 NB @ Eastwood TFWY   39.032335 -94.500228\n",
            "1   I-170 EB @ Before Sterling ave  39.047574 -94.457909\n",
            "2     I-170 WB @ Before Blue Ridge  39.047839 -94.454545\n",
            "3    I70 WB @ Before 40/Blue Ridge  39.046683 -94.425800\n",
            "4    I70 EB @ Before Lee Summit Rd  39.045510 -94.407959\n",
            "5         I-435 SB @ Before 40 Hwy  39.072685 -94.488767\n",
            "6     I-70 WB @ Before Jackson ave  39.071902 -94.522787\n",
            "7  I-70 EB at Before 40/Manchester  39.069813 -94.514621\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import folium\n",
        "import requests\n",
        "import pandas as pd\n",
        "from polyline import decode as decode_polyline\n",
        "\n",
        "# Function to load the API key securely from Google Drive\n",
        "def load_api_key(filepath):\n",
        "    with open(filepath, 'r') as f:\n",
        "        for line in f:\n",
        "            key, value = line.strip().split('=')\n",
        "            if key == 'api_key':\n",
        "                return value\n",
        "\n",
        "# Fetch route details from Google Maps Directions API\n",
        "def fetch_routes(api_key, origin, destination):\n",
        "    url = \"https://maps.googleapis.com/maps/api/directions/json\"\n",
        "    params = {\n",
        "        \"origin\": origin,\n",
        "        \"destination\": destination,\n",
        "        \"key\": api_key,\n",
        "        \"alternatives\": \"true\"  # Enable fetching alternative routes\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        if data['status'] == 'OK':\n",
        "            # Extract all routes\n",
        "            routes = []\n",
        "            for route in data['routes']:\n",
        "                polyline = route['overview_polyline']['points']\n",
        "                routes.append(decode_polyline(polyline))  # Decode the polyline into lat/lng points\n",
        "            return routes\n",
        "        else:\n",
        "            print(f\"Error from API: {data['status']}\")\n",
        "    else:\n",
        "        print(f\"HTTP Error: {response.status_code}\")\n",
        "    return None\n",
        "\n",
        "# Plot routes on a map\n",
        "def plot_routes_on_map(api_key, data):\n",
        "    # Initialize the map\n",
        "    m = folium.Map(location=[39.048786, -94.484566], zoom_start=13)\n",
        "    colors = ['blue', 'green', 'purple', 'orange', 'red']  # Colors for multiple routes\n",
        "\n",
        "    for index, row in data.iterrows():\n",
        "        origin = f\"{row['origin_lat']},{row['origin_lng']}\"\n",
        "        destination = f\"{row['destination_lat']},{row['destination_lng']}\"\n",
        "\n",
        "        # Fetch all routes data\n",
        "        routes = fetch_routes(api_key, origin, destination)\n",
        "        if routes:\n",
        "            for i, route_coords in enumerate(routes):\n",
        "                # Cycle through colors if there are more routes than colors\n",
        "                color = colors[i % len(colors)]\n",
        "                # Add the real route to the map\n",
        "                folium.PolyLine(route_coords, color=color, weight=2.5, opacity=0.7,\n",
        "                                tooltip=f\"Route {i + 1}\").add_to(m)\n",
        "\n",
        "            # Add markers for origin and destination\n",
        "            folium.Marker(\n",
        "                location=[row['origin_lat'], row['origin_lng']],\n",
        "                popup=f\"Origin: {row['origin']}\",\n",
        "                icon=folium.Icon(color='blue', icon='info-sign')\n",
        "            ).add_to(m)\n",
        "\n",
        "            folium.Marker(\n",
        "                location=[row['destination_lat'], row['destination_lng']],\n",
        "                popup=f\"Destination: {row['destination']}\",\n",
        "                icon=folium.Icon(color='red', icon='info-sign')\n",
        "            ).add_to(m)\n",
        "\n",
        "    return m\n",
        "\n",
        "# Main script\n",
        "if __name__ == \"__main__\":\n",
        "    # Path to the file containing your Google Maps API key on Google Drive\n",
        "    api_key_path = \"/content/drive/MyDrive/API_K.txt\"  # Update with your actual file path\n",
        "\n",
        "    # Load the API key\n",
        "    api_key = load_api_key(api_key_path)\n",
        "\n",
        "    # Load the dataset (replace with the actual dataset path)\n",
        "    dataset_path = \"/content/drive/MyDrive/chiefs-bills.xlsx\" # Update with your dataset path\n",
        "    df = pd.read_excel(dataset_path)\n",
        "\n",
        "    # Parse the `query` column to extract origin and destination coordinates\n",
        "    df[['origin_lat', 'origin_lng', 'destination_lat', 'destination_lng']] = df['query'].str.split(', ', expand=True)\n",
        "    df['origin_lat'] = df['origin_lat'].astype(float)\n",
        "    df['origin_lng'] = df['origin_lng'].astype(float)\n",
        "    df['destination_lat'] = df['destination_lat'].astype(float)\n",
        "    df['destination_lng'] = df['destination_lng'].astype(float)\n",
        "\n",
        "    # Call the function to plot the routes on a map\n",
        "    traffic_map = plot_routes_on_map(api_key, df)\n",
        "\n",
        "    # Save the map to an HTML file\n",
        "    traffic_map.save(\"traffic_routes_map24.html\")\n",
        "    print(\"Map saved as traffic_routes_map24.html\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FtOMAz0gcg_",
        "outputId": "cd1b043c-5d98-4eff-9753-a6eb3473a273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Map saved as traffic_routes_map24.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import folium\n",
        "import requests\n",
        "import pandas as pd\n",
        "from polyline import decode as decode_polyline\n",
        "from datetime import datetime, timedelta\n",
        "from folium.plugins import HeatMap\n",
        "\n",
        "# Function to load the API key securely from Google Drive\n",
        "def load_api_key(filepath):\n",
        "    with open(filepath, 'r') as f:\n",
        "        for line in f:\n",
        "            key, value = line.strip().split('=')\n",
        "            if key == 'api_key':\n",
        "                return value\n",
        "\n",
        "# Convert UTC time to Central Time (CT)\n",
        "def convert_to_central_time(utc_time):\n",
        "    # If the input is an integer (Unix timestamp)\n",
        "    if isinstance(utc_time, int):\n",
        "        utc_dt = datetime.utcfromtimestamp(utc_time)  # Convert Unix timestamp to datetime\n",
        "    else:  # If the input is a string\n",
        "        utc_dt = datetime.strptime(utc_time, \"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    # Convert to Central Time (UTC-6)\n",
        "    central_dt = utc_dt - timedelta(hours=6)\n",
        "    return central_dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "# Fetch route details from Google Maps Directions API\n",
        "def fetch_routes(api_key, origin, destination):\n",
        "    url = \"https://maps.googleapis.com/maps/api/directions/json\"\n",
        "    params = {\n",
        "        \"origin\": origin,\n",
        "        \"destination\": destination,\n",
        "        \"key\": api_key,\n",
        "        \"alternatives\": \"true\"  # Enable fetching alternative routes\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        if data['status'] == 'OK':\n",
        "            # Extract all routes\n",
        "            routes = []\n",
        "            for route in data['routes']:\n",
        "                polyline = route['overview_polyline']['points']\n",
        "                routes.append(decode_polyline(polyline))  # Decode the polyline into lat/lng points\n",
        "            return routes\n",
        "        else:\n",
        "            print(f\"Error from API: {data['status']}\")\n",
        "    else:\n",
        "        print(f\"HTTP Error: {response.status_code}\")\n",
        "    return None\n",
        "\n",
        "# Create a heatmap based on routes and time intervals\n",
        "def create_route_heatmap(data, api_key, time_interval):\n",
        "    # Filter data by the specified time interval\n",
        "    filtered_data = data[data['central_time'].str.contains('|'.join(time_interval))]\n",
        "\n",
        "    # Initialize the map\n",
        "    m = folium.Map(location=[39.048786, -94.484566], zoom_start=13)\n",
        "\n",
        "    # Extract route coordinates for the heatmap\n",
        "    heat_data = []\n",
        "    for index, row in filtered_data.iterrows():\n",
        "        origin = f\"{row['origin_lat']},{row['origin_lng']}\"\n",
        "        destination = f\"{row['destination_lat']},{row['destination_lng']}\"\n",
        "\n",
        "        # Fetch all routes data\n",
        "        routes = fetch_routes(api_key, origin, destination)\n",
        "        if routes:\n",
        "            for route_coords in routes:\n",
        "                for lat, lng in route_coords:\n",
        "                    heat_data.append([lat, lng, 1])  # Add each point with a weight of 1\n",
        "\n",
        "    # Add HeatMap layer\n",
        "    HeatMap(heat_data).add_to(m)\n",
        "\n",
        "    return m\n",
        "\n",
        "# Main script\n",
        "if __name__ == \"__main__\":\n",
        "    # Path to the file containing your Google Maps API key on Google Drive\n",
        "    api_key_path = \"/content/drive/MyDrive/API_K.txt\"  # Update with your actual file path\n",
        "\n",
        "    # Load the API key\n",
        "    api_key = load_api_key(api_key_path)\n",
        "\n",
        "    # Load the dataset (replace with the actual dataset path)\n",
        "    dataset_path = \"/content/drive/MyDrive/chiefs-bills.xlsx\"  # Update with your dataset path\n",
        "    df = pd.read_excel(dataset_path)\n",
        "\n",
        "    # Parse the `query` column to extract origin and destination coordinates\n",
        "    df[['origin_lat', 'origin_lng', 'destination_lat', 'destination_lng']] = df['query'].str.split(', ', expand=True)\n",
        "    df['origin_lat'] = df['origin_lat'].astype(float)\n",
        "    df['origin_lng'] = df['origin_lng'].astype(float)\n",
        "    df['destination_lat'] = df['destination_lat'].astype(float)\n",
        "    df['destination_lng'] = df['destination_lng'].astype(float)\n",
        "\n",
        "    # Convert UTC timestamps to Central Time if a timestamp column exists\n",
        "    if 'timestamp' in df.columns:\n",
        "        df['central_time'] = df['timestamp'].apply(convert_to_central_time)\n",
        "\n",
        "    # Define the time interval for analysis\n",
        "    time_interval = \"12:30\", \"14:30\"\n",
        "\n",
        "\n",
        "    # Create the heatmap for the specified time interval\n",
        "    heatmap = create_route_heatmap(df, api_key, time_interval)\n",
        "\n",
        "    # Save the map to an HTML file\n",
        "    heatmap.save(\"traffic_route_heatmap2.html\")\n",
        "    print(\"Map saved as traffic_route_heatmap2.html\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65-q6ujgn8m9",
        "outputId": "7ad026a5-7440-4fcc-f21d-9189d28aa2ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Map saved as traffic_route_heatmap2.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import folium\n",
        "import requests\n",
        "import pandas as pd\n",
        "from polyline import decode as decode_polyline\n",
        "from datetime import datetime, timedelta\n",
        "from folium.plugins import HeatMap\n",
        "\n",
        "# Function to load the API key securely from Google Drive\n",
        "def load_api_key(filepath):\n",
        "    with open(filepath, 'r') as f:\n",
        "        for line in f:\n",
        "            key, value = line.strip().split('=')\n",
        "            if key == 'api_key':\n",
        "                return value\n",
        "\n",
        "# Convert UTC time to Central Time (CT)\n",
        "def convert_to_central_time(utc_time):\n",
        "    # If the input is an integer (Unix timestamp)\n",
        "    if isinstance(utc_time, int):\n",
        "        utc_dt = datetime.utcfromtimestamp(utc_time)  # Convert Unix timestamp to datetime\n",
        "    else:  # If the input is a string\n",
        "        utc_dt = datetime.strptime(utc_time, \"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    # Convert to Central Time (UTC-6)\n",
        "    central_dt = utc_dt - timedelta(hours=6)\n",
        "    return central_dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "# Fetch route details from Google Maps Directions API\n",
        "def fetch_routes(api_key, origin, destination):\n",
        "    url = \"https://maps.googleapis.com/maps/api/directions/json\"\n",
        "    params = {\n",
        "        \"origin\": origin,\n",
        "        \"destination\": destination,\n",
        "        \"key\": api_key,\n",
        "        \"alternatives\": \"true\"  # Enable fetching alternative routes\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        if data['status'] == 'OK':\n",
        "            # Extract all routes\n",
        "            routes = []\n",
        "            for route in data['routes']:\n",
        "                polyline = route['overview_polyline']['points']\n",
        "                routes.append(decode_polyline(polyline))  # Decode the polyline into lat/lng points\n",
        "            return routes\n",
        "        else:\n",
        "            print(f\"Error from API: {data['status']}\")\n",
        "    else:\n",
        "        print(f\"HTTP Error: {response.status_code}\")\n",
        "    return None\n",
        "\n",
        "# Create a heatmap based on routes and time intervals\n",
        "def create_route_heatmap(data, api_key, time_interval):\n",
        "    # Filter data by the specified time interval\n",
        "    filtered_data = data[data['central_time'].str.contains('|'.join(time_interval))]\n",
        "\n",
        "    # Initialize the map\n",
        "    m = folium.Map(location=[39.048786, -94.484566], zoom_start=13)\n",
        "\n",
        "    # Extract route coordinates for the heatmap\n",
        "    heat_data = []\n",
        "    for index, row in filtered_data.iterrows():\n",
        "        origin = f\"{row['origin_lat']},{row['origin_lng']}\"\n",
        "        destination = f\"{row['destination_lat']},{row['destination_lng']}\"\n",
        "\n",
        "        # Fetch all routes data\n",
        "        routes = fetch_routes(api_key, origin, destination)\n",
        "        if routes:\n",
        "            for route_coords in routes:\n",
        "                for lat, lng in route_coords:\n",
        "                    heat_data.append([lat, lng, 1])  # Add each point with a weight of 1\n",
        "\n",
        "    # Add HeatMap layer\n",
        "    HeatMap(heat_data).add_to(m)\n",
        "\n",
        "    return m\n",
        "\n",
        "# Main script\n",
        "if __name__ == \"__main__\":\n",
        "    # Path to the file containing your Google Maps API key on Google Drive\n",
        "    api_key_path = \"/content/drive/MyDrive/API_K.txt\"  # Update with your actual file path\n",
        "\n",
        "    # Load the API key\n",
        "    api_key = load_api_key(api_key_path)\n",
        "\n",
        "    # Load the dataset (replace with the actual dataset path)\n",
        "    dataset_path = \"/content/drive/MyDrive/chiefs-bills.xlsx\"  # Update with your dataset path\n",
        "    df = pd.read_excel(dataset_path)\n",
        "\n",
        "    # Parse the `query` column to extract origin and destination coordinates\n",
        "    df[['origin_lat', 'origin_lng', 'destination_lat', 'destination_lng']] = df['query'].str.split(', ', expand=True)\n",
        "    df['origin_lat'] = df['origin_lat'].astype(float)\n",
        "    df['origin_lng'] = df['origin_lng'].astype(float)\n",
        "    df['destination_lat'] = df['destination_lat'].astype(float)\n",
        "    df['destination_lng'] = df['destination_lng'].astype(float)\n",
        "\n",
        "    # Convert UTC timestamps to Central Time if a timestamp column exists\n",
        "    if 'timestamp' in df.columns:\n",
        "        df['central_time'] = df['timestamp'].apply(convert_to_central_time)\n",
        "\n",
        "    # Define the time interval for analysis\n",
        "    time_interval = \"14:30\", \"16:30\"\n",
        "\n",
        "\n",
        "    # Create the heatmap for the specified time interval\n",
        "    heatmap = create_route_heatmap(df, api_key, time_interval)\n",
        "\n",
        "    # Save the map to an HTML file\n",
        "    heatmap.save(\"traffic_route_heatmap3.html\")\n",
        "    print(\"Map saved as traffic_route_heatmap3.html\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e81b60f1-2689-4cf8-d95e-d2258989c407",
        "id": "gOfBPhNMpARe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Map saved as traffic_route_heatmap3.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import folium\n",
        "import requests\n",
        "import pandas as pd\n",
        "from polyline import decode as decode_polyline\n",
        "from datetime import datetime, timedelta\n",
        "from folium.plugins import HeatMap\n",
        "\n",
        "# Function to load the API key securely from Google Drive\n",
        "def load_api_key(filepath):\n",
        "    with open(filepath, 'r') as f:\n",
        "        for line in f:\n",
        "            key, value = line.strip().split('=')\n",
        "            if key == 'api_key':\n",
        "                return value\n",
        "\n",
        "# Convert UTC time to Central Time (CT)\n",
        "def convert_to_central_time(utc_time):\n",
        "    # If the input is an integer (Unix timestamp)\n",
        "    if isinstance(utc_time, int):\n",
        "        utc_dt = datetime.utcfromtimestamp(utc_time)  # Convert Unix timestamp to datetime\n",
        "    else:  # If the input is a string\n",
        "        utc_dt = datetime.strptime(utc_time, \"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    # Convert to Central Time (UTC-6)\n",
        "    central_dt = utc_dt - timedelta(hours=6)\n",
        "    return central_dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "# Fetch route details from Google Maps Directions API\n",
        "def fetch_routes(api_key, origin, destination):\n",
        "    url = \"https://maps.googleapis.com/maps/api/directions/json\"\n",
        "    params = {\n",
        "        \"origin\": origin,\n",
        "        \"destination\": destination,\n",
        "        \"key\": api_key,\n",
        "        \"alternatives\": \"true\"  # Enable fetching alternative routes\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        if data['status'] == 'OK':\n",
        "            # Extract all routes\n",
        "            routes = []\n",
        "            for route in data['routes']:\n",
        "                polyline = route['overview_polyline']['points']\n",
        "                routes.append(decode_polyline(polyline))  # Decode the polyline into lat/lng points\n",
        "            return routes\n",
        "        else:\n",
        "            print(f\"Error from API: {data['status']}\")\n",
        "    else:\n",
        "        print(f\"HTTP Error: {response.status_code}\")\n",
        "    return None\n",
        "\n",
        "# Create a heatmap based on routes and time intervals\n",
        "def create_route_heatmap(data, api_key, time_interval):\n",
        "    # Filter data by the specified time interval\n",
        "    filtered_data = data[data['central_time'].str.contains('|'.join(time_interval))]\n",
        "\n",
        "    # Initialize the map\n",
        "    m = folium.Map(location=[39.048786, -94.484566], zoom_start=13)\n",
        "\n",
        "    # Extract route coordinates for the heatmap\n",
        "    heat_data = []\n",
        "    for index, row in filtered_data.iterrows():\n",
        "        origin = f\"{row['origin_lat']},{row['origin_lng']}\"\n",
        "        destination = f\"{row['destination_lat']},{row['destination_lng']}\"\n",
        "\n",
        "        # Fetch all routes data\n",
        "        routes = fetch_routes(api_key, origin, destination)\n",
        "        if routes:\n",
        "            for route_coords in routes:\n",
        "                for lat, lng in route_coords:\n",
        "                    heat_data.append([lat, lng, 1])  # Add each point with a weight of 1\n",
        "\n",
        "    # Add HeatMap layer\n",
        "    HeatMap(heat_data).add_to(m)\n",
        "\n",
        "    return m\n",
        "\n",
        "# Main script\n",
        "if __name__ == \"__main__\":\n",
        "    # Path to the file containing your Google Maps API key on Google Drive\n",
        "    api_key_path = \"/content/drive/MyDrive/API_K.txt\"  # Update with your actual file path\n",
        "\n",
        "    # Load the API key\n",
        "    api_key = load_api_key(api_key_path)\n",
        "\n",
        "    # Load the dataset (replace with the actual dataset path)\n",
        "    dataset_path = \"/content/drive/MyDrive/chiefs-bills.xlsx\"  # Update with your dataset path\n",
        "    df = pd.read_excel(dataset_path)\n",
        "\n",
        "    # Parse the `query` column to extract origin and destination coordinates\n",
        "    df[['origin_lat', 'origin_lng', 'destination_lat', 'destination_lng']] = df['query'].str.split(', ', expand=True)\n",
        "    df['origin_lat'] = df['origin_lat'].astype(float)\n",
        "    df['origin_lng'] = df['origin_lng'].astype(float)\n",
        "    df['destination_lat'] = df['destination_lat'].astype(float)\n",
        "    df['destination_lng'] = df['destination_lng'].astype(float)\n",
        "\n",
        "    # Convert UTC timestamps to Central Time if a timestamp column exists\n",
        "    if 'timestamp' in df.columns:\n",
        "        df['central_time'] = df['timestamp'].apply(convert_to_central_time)\n",
        "\n",
        "    # Define the time interval for analysis\n",
        "    time_interval = \"16:30\", \"17:30\"\n",
        "\n",
        "\n",
        "    # Create the heatmap for the specified time interval\n",
        "    heatmap = create_route_heatmap(df, api_key, time_interval)\n",
        "\n",
        "    # Save the map to an HTML file\n",
        "    heatmap.save(\"traffic_route_heatmap4.html\")\n",
        "    print(\"Map saved as traffic_route_heatmap4.html\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7267e085-3777-48d0-a7d0-f81f150ff1f2",
        "id": "b8iceuZXpKrb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Map saved as traffic_route_heatmap4.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import folium\n",
        "import requests\n",
        "import pandas as pd\n",
        "from polyline import decode as decode_polyline\n",
        "from pytz import timezone\n",
        "from datetime import datetime\n",
        "from haversine import haversine, Unit\n",
        "import json\n",
        "\n",
        "\n",
        "# Function to load the API key securely from Google Drive\n",
        "def load_api_key(filepath):\n",
        "    with open(filepath, 'r') as f:\n",
        "        for line in f:\n",
        "            key, value = line.strip().split('=')\n",
        "            if key == 'api_key':\n",
        "                return value\n",
        "\n",
        "# Fetch route details from Google Maps Directions API\n",
        "def fetch_route_polyline(api_key, origin, destination):\n",
        "    url = \"https://maps.googleapis.com/maps/api/directions/json\"\n",
        "    params = {\n",
        "        \"origin\": origin,\n",
        "        \"destination\": destination,\n",
        "        \"key\": api_key,\n",
        "        \"alternatives\": \"true\"  # Enable fetching alternative routes\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        if data['status'] == 'OK':\n",
        "            return data['routes'][0]['overview_polyline']['points']  # Return the encoded polyline\n",
        "        else:\n",
        "            print(f\"Error from API: {data['status']}\")\n",
        "    else:\n",
        "        print(f\"HTTP Error: {response.status_code}\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# Convert UTC times to Central Time\n",
        "def convert_to_central_time(utc_time_str):\n",
        "    utc = timezone('UTC')\n",
        "    central = timezone('US/Central')\n",
        "    utc_time = datetime.strptime(utc_time_str, \"%m/%d/%Y %H:%M:%S\")\n",
        "    central_time = utc.localize(utc_time).astimezone(central)\n",
        "    return central_time.strftime(\"%m/%d/%Y %H:%M:%S\")\n",
        "\n",
        "# Analyze congestion flags\n",
        "def update_congestion_flags(speeds_mph):\n",
        "    return [speed < 20 if speed is not None else False for speed in speeds_mph]\n",
        "\n",
        "# Find lat/long of congested points based on distance along polyline\n",
        "def get_congestion_coordinates(route_polyline, distances_miles):\n",
        "    route_coords = decode_polyline(route_polyline)\n",
        "    congestion_coords = []\n",
        "    total_distance = 0\n",
        "\n",
        "    for i in range(1, len(route_coords)):\n",
        "        # Calculate segment distance in miles\n",
        "        dist = haversine(route_coords[i - 1], route_coords[i])\n",
        "        total_distance += dist\n",
        "\n",
        "        # Match congestion distances to coordinates\n",
        "        for distance in distances_miles:\n",
        "            if total_distance >= distance:\n",
        "                congestion_coords.append(route_coords[i])\n",
        "                distances_miles.remove(distance)\n",
        "                break\n",
        "        if not distances_miles:\n",
        "            break\n",
        "    return congestion_coords\n",
        "\n",
        "# Main script\n",
        "if __name__ == \"__main__\":\n",
        "    # Set file paths\n",
        "    api_key_path = \"/content/drive/MyDrive/API_K.txt\"  # directions API\n",
        "    dataset_path = \"/content/drive/MyDrive/chiefs-bills.xlsx\"\n",
        "\n",
        "    # Load the API key\n",
        "    api_key = load_api_key(api_key_path)\n",
        "\n",
        "    # Load the dataset\n",
        "    df = pd.read_excel(dataset_path)\n",
        "\n",
        "    # Parse and extract route polylines for all rows\n",
        "    df['origin'] = df['origin_coordinates']\n",
        "    df['destination'] = df['destination_coordinates']\n",
        "    df['polyline'] = df.apply(\n",
        "        lambda row: fetch_route_polyline(api_key, row['origin'], row['destination']), axis=1\n",
        "    )\n",
        "\n",
        "    # Parse road timing data\n",
        "    df['road_timing_parsed'] = df['road_distance_timing(meters:seconds)'].apply(\n",
        "        lambda x: json.loads(x.replace(\"'\", '\"'))\n",
        "    )\n",
        "\n",
        "    # Convert road timing data and calculate speeds safely\n",
        "    df['speeds_mph'], df['congestion_flags_20mph'] = zip(\n",
        "    *df['road_timing_parsed'].apply(\n",
        "        lambda timing: (\n",
        "            [pd.to_numeric(dist, errors='coerce') * 0.000621371 / (pd.to_numeric(time, errors='coerce') / 3600)\n",
        "             if pd.to_numeric(time, errors='coerce') > 0 else None\n",
        "             for dist, time in timing.items()],\n",
        "            [pd.to_numeric(dist, errors='coerce') * 0.000621371 / (pd.to_numeric(time, errors='coerce') / 3600) < 40\n",
        "             if pd.to_numeric(time, errors='coerce') > 0 else False\n",
        "             for dist, time in timing.items()]\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "  # Extract congestion details\n",
        "    df['congestion_details'] = df.apply(\n",
        "    lambda row: [\n",
        "        (convert_to_central_time(row['datetime_utc']), pd.to_numeric(dist, errors='coerce') * 0.000621371)\n",
        "        for dist, flag in zip(row['road_timing_parsed'].keys(), row['congestion_flags_20mph'])\n",
        "        if flag and pd.to_numeric(dist, errors='coerce') is not None\n",
        "    ],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "    # Find coordinates for congestion points\n",
        "    df['congestion_coords'] = df.apply(\n",
        "        lambda row: get_congestion_coordinates(row['polyline'], [dist for _, dist in row['congestion_details']]),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "  # Map congestion points\n",
        "    congestion_map = folium.Map(location=[39.048786, -94.484566], zoom_start=12)\n",
        "\n",
        "\n",
        "# Add small red dots for congestion points\n",
        "for idx, row in df.iterrows():\n",
        "    if 'congestion_coords' in row and 'congestion_details' in row:\n",
        "        for coord, detail in zip(row['congestion_coords'], row['congestion_details']):\n",
        "            time, distance = detail\n",
        "            folium.CircleMarker(\n",
        "                location=coord,\n",
        "                radius=5,  # Size of the dot\n",
        "                color=\"red\",\n",
        "                fill=True,\n",
        "                fill_color=\"red\",\n",
        "                fill_opacity=0.8,\n",
        "                popup=f\"<b>Congestion Details</b><br>Distance: {distance:.2f} miles<br>Time: {time}\"\n",
        "            ).add_to(congestion_map)\n",
        "\n",
        "# Save the map with red dots\n",
        "congestion_map.save(\"congestion_map_with_dots1.html\")\n",
        "\"Congestion map with red dots saved as 'congestion_map_with_dots1.html'.\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MfLnjYlLDDSK",
        "outputId": "08da852a-751f-4b98-e9df-0e76dfa0838d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Congestion map with red dots saved as 'congestion_map_with_dots1.html'.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import folium\n",
        "import requests\n",
        "import pandas as pd\n",
        "from polyline import decode as decode_polyline\n",
        "from pytz import timezone\n",
        "from datetime import datetime\n",
        "from haversine import haversine\n",
        "import json\n",
        "\n",
        "# Function to load the API key securely\n",
        "def load_api_key(filepath):\n",
        "    with open(filepath, 'r') as f:\n",
        "        for line in f:\n",
        "            key, value = line.strip().split('=')\n",
        "            if key == 'api_key':\n",
        "                return value\n",
        "\n",
        "# Fetch route details from Google Maps Directions API\n",
        "def fetch_routes(api_key, origin, destination):\n",
        "    url = \"https://maps.googleapis.com/maps/api/directions/json\"\n",
        "    params = {\n",
        "        \"origin\": origin,\n",
        "        \"destination\": destination,\n",
        "        \"key\": api_key,\n",
        "        \"alternatives\": \"true\"\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        if data['status'] == 'OK':\n",
        "            routes = []\n",
        "            for route in data['routes']:\n",
        "                polyline = route['overview_polyline']['points']\n",
        "                routes.append(decode_polyline(polyline))\n",
        "            return routes\n",
        "        else:\n",
        "            print(f\"Error from API: {data['status']}\")\n",
        "    else:\n",
        "        print(f\"HTTP Error: {response.status_code}\")\n",
        "    return None\n",
        "\n",
        "# Convert UTC times to Central Time\n",
        "def convert_to_central_time(utc_time_str):\n",
        "    utc = timezone('UTC')\n",
        "    central = timezone('US/Central')\n",
        "    utc_time = datetime.strptime(utc_time_str, \"%m/%d/%Y %H:%M:%S\")\n",
        "    central_time = utc.localize(utc_time).astimezone(central)\n",
        "    return central_time.strftime(\"%m/%d/%Y %H:%M:%S\")\n",
        "\n",
        "# Analyze congestion flags\n",
        "def update_congestion_flags(speeds_mph):\n",
        "    return [speed < 20 if speed is not None else False for speed in speeds_mph]\n",
        "\n",
        "# Find lat/long of congested points\n",
        "def get_congestion_coordinates(route_polyline, distances_miles):\n",
        "    route_coords = decode_polyline(route_polyline)\n",
        "    congestion_coords = []\n",
        "    total_distance = 0\n",
        "\n",
        "    for i in range(1, len(route_coords)):\n",
        "        dist = haversine(route_coords[i - 1], route_coords[i])\n",
        "        total_distance += dist\n",
        "        for distance in distances_miles:\n",
        "            if total_distance >= distance:\n",
        "                congestion_coords.append(route_coords[i])\n",
        "                distances_miles.remove(distance)\n",
        "                break\n",
        "        if not distances_miles:\n",
        "            break\n",
        "    return congestion_coords\n",
        "\n",
        "# Plot routes and congestion on the map\n",
        "def plot_routes_and_congestion(api_key, data):\n",
        "    m = folium.Map(location=[39.048786, -94.484566], zoom_start=12)\n",
        "    colors = ['blue', 'green', 'purple', 'orange', 'red']\n",
        "\n",
        "    for index, row in data.iterrows():\n",
        "        origin = f\"{row['origin_lat']},{row['origin_lng']}\"\n",
        "        destination = f\"{row['destination_lat']},{row['destination_lng']}\"\n",
        "        routes = fetch_routes(api_key, origin, destination)\n",
        "\n",
        "        if routes:\n",
        "            for i, route_coords in enumerate(routes):\n",
        "                color = colors[i % len(colors)]\n",
        "                folium.PolyLine(route_coords, color=color, weight=4, opacity=0.9,\n",
        "                                tooltip=f\"Route {i + 1}\").add_to(m)\n",
        "\n",
        "        if 'congestion_coords' in row and 'congestion_details' in row:\n",
        "            for coord, detail in zip(row['congestion_coords'], row['congestion_details']):\n",
        "                time, distance = detail\n",
        "                folium.CircleMarker(\n",
        "                    location=coord,\n",
        "                    radius=5,\n",
        "                    color=\"red\",\n",
        "                    fill=True,\n",
        "                    fill_color=\"red\",\n",
        "                    fill_opacity=0.8,\n",
        "                    popup=f\"<b>Congestion Details</b><br>Distance: {distance:.2f} miles<br>Time: {time}\"\n",
        "                ).add_to(m)\n",
        "\n",
        "        # Add origin and destination markers\n",
        "        folium.Marker(\n",
        "            location=[row['origin_lat'], row['origin_lng']],\n",
        "            popup=f\"Origin: {row['origin']}\",\n",
        "            icon=folium.Icon(color='blue', icon='info-sign')\n",
        "        ).add_to(m)\n",
        "        folium.Marker(\n",
        "            location=[row['destination_lat'], row['destination_lng']],\n",
        "            popup=f\"Destination: {row['destination']}\",\n",
        "            icon=folium.Icon(color='red', icon='info-sign')\n",
        "        ).add_to(m)\n",
        "\n",
        "    return m\n",
        "\n",
        "# Main script\n",
        "if __name__ == \"__main__\":\n",
        "    api_key_path = \"/content/drive/MyDrive/API_K.txt\"\n",
        "    dataset_path = \"/content/drive/MyDrive/chiefs-bills.xlsx\"\n",
        "\n",
        "    # Load API key\n",
        "    api_key = load_api_key(api_key_path)\n",
        "\n",
        "    # Load the dataset\n",
        "    df = pd.read_excel(dataset_path)\n",
        "    df[['origin_lat', 'origin_lng', 'destination_lat', 'destination_lng']] = df['query'].str.split(', ', expand=True)\n",
        "    df['origin_lat'] = df['origin_lat'].astype(float)\n",
        "    df['origin_lng'] = df['origin_lng'].astype(float)\n",
        "    df['destination_lat'] = df['destination_lat'].astype(float)\n",
        "    df['destination_lng'] = df['destination_lng'].astype(float)\n",
        "\n",
        " # Fetch route details (polyline) before processing congestion\n",
        "    df['polyline'] = df.apply(\n",
        "        lambda row: fetch_route_polyline(api_key, f\"{row['origin_lat']},{row['origin_lng']}\",\n",
        "                                         f\"{row['destination_lat']},{row['destination_lng']}\"), axis=1\n",
        "    )\n",
        "\n",
        "    # Parse road timing data\n",
        "    df['road_timing_parsed'] = df['road_distance_timing(meters:seconds)'].apply(\n",
        "        lambda x: json.loads(x.replace(\"'\", '\"'))\n",
        "    )\n",
        "    df['speeds_mph'], df['congestion_flags_20mph'] = zip(\n",
        "        *df['road_timing_parsed'].apply(\n",
        "            lambda timing: (\n",
        "                [pd.to_numeric(dist, errors='coerce') * 0.000621371 / (pd.to_numeric(time, errors='coerce') / 3600)\n",
        "                 if pd.to_numeric(time, errors='coerce') > 0 else None\n",
        "                 for dist, time in timing.items()],\n",
        "                [pd.to_numeric(dist, errors='coerce') * 0.000621371 / (pd.to_numeric(time, errors='coerce') / 3600) < 40\n",
        "                 if pd.to_numeric(time, errors='coerce') > 0 else False\n",
        "                 for dist, time in timing.items()]\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Extract congestion details\n",
        "    df['congestion_details'] = df.apply(\n",
        "        lambda row: [\n",
        "            (convert_to_central_time(row['datetime_utc']), pd.to_numeric(dist, errors='coerce') * 0.000621371)\n",
        "            for dist, flag in zip(row['road_timing_parsed'].keys(), row['congestion_flags_20mph'])\n",
        "            if flag and pd.to_numeric(dist, errors='coerce') is not None\n",
        "        ],\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Find congestion coordinates\n",
        "    df['congestion_coords'] = df.apply(\n",
        "        lambda row: get_congestion_coordinates(row['polyline'], [dist for _, dist in row['congestion_details']]),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Plot everything\n",
        "    traffic_map = plot_routes_and_congestion(api_key, df)\n",
        "    traffic_map.save(\"combined_map.html\")\n",
        "    print(\"Combined map saved as 'combined_map.html'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHGUbxhle9Fp",
        "outputId": "c72eb6a5-5a54-45ad-c98d-7e8cb960c147"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined map saved as 'combined_map.html'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import folium\n",
        "import requests\n",
        "import pandas as pd\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "from zipfile import ZipFile\n",
        "from polyline import decode as decode_polyline\n",
        "from pytz import timezone\n",
        "from datetime import datetime\n",
        "from haversine import haversine\n",
        "\n",
        "# File paths\n",
        "api_key_path = \"/content/drive/MyDrive/API_K.txt\"\n",
        "traffic_data_file = \"/content/drive/MyDrive/chiefs-bills.xlsx\"\n",
        "kmz_file = \"/content/drive/MyDrive/inv.kmz\"\n",
        "\n",
        "# Function to load API key securely\n",
        "def load_api_key(filepath):\n",
        "    with open(filepath, 'r') as f:\n",
        "        for line in f:\n",
        "            key, value = line.strip().split('=')\n",
        "            if key == 'api_key':\n",
        "                return value\n",
        "\n",
        "api_key = load_api_key(api_key_path)\n",
        "\n",
        "# Function to fetch routes from Google Maps API\n",
        "def fetch_routes(api_key, origin, destination):\n",
        "    url = \"https://maps.googleapis.com/maps/api/directions/json\"\n",
        "    params = {\"origin\": origin, \"destination\": destination, \"key\": api_key, \"alternatives\": \"true\"}\n",
        "    response = requests.get(url, params=params)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        if data['status'] == 'OK':\n",
        "            return [decode_polyline(route['overview_polyline']['points']) for route in data['routes']]\n",
        "        else:\n",
        "            print(f\"API Error: {data['status']}\")\n",
        "    else:\n",
        "        print(f\"HTTP Error: {response.status_code}\")\n",
        "    return None\n",
        "\n",
        "# Function to convert UTC time to Central Time\n",
        "def convert_to_central_time(utc_time_str):\n",
        "    utc = timezone('UTC')\n",
        "    central = timezone('US/Central')\n",
        "    utc_time = datetime.strptime(utc_time_str, \"%m/%d/%Y %H:%M:%S\")\n",
        "    central_time = utc.localize(utc_time).astimezone(central)\n",
        "    return central_time.strftime(\"%m/%d/%Y %H:%M:%S\")\n",
        "\n",
        "# Function to detect congestion based on speed thresholds\n",
        "def update_congestion_flags(speeds_mph, threshold=40):\n",
        "    return [speed is not None and speed < threshold for speed in speeds_mph]\n",
        "\n",
        "# Function to extract congestion coordinates\n",
        "def get_congestion_coordinates(polyline_str, distances):\n",
        "    # Only decode the polyline if it's a string\n",
        "    if isinstance(polyline_str, str):\n",
        "        coordinates = polyline.decode(polyline_str)\n",
        "    else:\n",
        "        coordinates = polyline_str  # Use the coordinates directly if already decoded\n",
        "\n",
        "    congestion_coords = []\n",
        "    total_distance = 0\n",
        "    for i, distance in enumerate(distances):\n",
        "        total_distance += distance\n",
        "        congestion_coords.append(coordinates[i])  # Add the coordinate corresponding to the distance\n",
        "\n",
        "    return congestion_coords\n",
        "\n",
        "\n",
        "# Extract cameras and DMS locations from the KMZ file\n",
        "kml_extracted_path = \"/content/drive/MyDrive/extracted_kml.kml\"\n",
        "with ZipFile(kmz_file, 'r') as kmz:\n",
        "    kml_filename = [file for file in kmz.namelist() if file.endswith('.kml')][0]\n",
        "    kmz.extract(kml_filename, path=\"/content/drive/MyDrive\")\n",
        "    kml_full_path = f\"/content/drive/MyDrive/{kml_filename}\"\n",
        "\n",
        "# Parse KML file\n",
        "namespace = {\"kml\": \"http://www.opengis.net/kml/2.2\"}\n",
        "tree = ET.parse(kml_full_path)\n",
        "root = tree.getroot()\n",
        "camera_dms_data = []\n",
        "for placemark in root.findall(\".//kml:Placemark\", namespace):\n",
        "    name = placemark.find(\"kml:name\", namespace).text\n",
        "    coords = placemark.find(\".//kml:coordinates\", namespace).text.strip()\n",
        "    lon, lat, _ = map(float, coords.split(\",\"))\n",
        "    camera_dms_data.append({\"Name\": name, \"Latitude\": lat, \"Longitude\": lon})\n",
        "camera_dms_df = pd.DataFrame(camera_dms_data)\n",
        "\n",
        "# Load traffic data\n",
        "df = pd.read_excel(traffic_data_file)\n",
        "df[['origin_lat', 'origin_lng', 'destination_lat', 'destination_lng']] = df['query'].str.split(', ', expand=True)\n",
        "df['origin_lat'] = df['origin_lat'].astype(float)\n",
        "df['origin_lng'] = df['origin_lng'].astype(float)\n",
        "df['destination_lat'] = df['destination_lat'].astype(float)\n",
        "df['destination_lng'] = df['destination_lng'].astype(float)\n",
        "\n",
        "# Fetch route details before processing congestion\n",
        "df['polyline'] = df.apply(\n",
        "    lambda row: fetch_routes(api_key, f\"{row['origin_lat']},{row['origin_lng']}\",\n",
        "                             f\"{row['destination_lat']},{row['destination_lng']}\"), axis=1\n",
        ")\n",
        "\n",
        "# Parse road timing data\n",
        "df['road_timing_parsed'] = df['road_distance_timing(meters:seconds)'].apply(\n",
        "    lambda x: json.loads(x.replace(\"'\", '\"'))\n",
        ")\n",
        "\n",
        "# Convert road timing data to speeds and detect congestion\n",
        "df['speeds_mph'], df['congestion_flags'] = zip(\n",
        "    *df['road_timing_parsed'].apply(\n",
        "        lambda timing: (\n",
        "            [pd.to_numeric(dist, errors='coerce') * 0.000621371 / (pd.to_numeric(time, errors='coerce') / 3600)\n",
        "             if pd.to_numeric(time, errors='coerce') > 0 else None\n",
        "             for dist, time in timing.items()],\n",
        "            update_congestion_flags(\n",
        "                [pd.to_numeric(dist, errors='coerce') * 0.000621371 / (pd.to_numeric(time, errors='coerce') / 3600)\n",
        "                 if pd.to_numeric(time, errors='coerce') > 0 else None\n",
        "                 for dist, time in timing.items()]\n",
        "            )\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "# Extract congestion details\n",
        "df['congestion_details'] = df.apply(\n",
        "    lambda row: [\n",
        "        (convert_to_central_time(row['datetime_utc']), pd.to_numeric(dist, errors='coerce') * 0.000621371)\n",
        "        for dist, flag in zip(row['road_timing_parsed'].keys(), row['congestion_flags'])\n",
        "        if flag and pd.to_numeric(dist, errors='coerce') is not None\n",
        "    ],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "df['congestion_coords'] = df.apply(\n",
        "    lambda row: get_congestion_coordinates(row['polyline'][0], [dist for _, dist in row['congestion_details']])\n",
        "    if row['polyline'] else [],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Initialize map\n",
        "traffic_map = folium.Map(location=[39.048786, -94.484566], zoom_start=12)\n",
        "\n",
        "# Add congestion points (red dots)\n",
        "for idx, row in df.iterrows():\n",
        "    if row['congestion_coords']:\n",
        "        for coord in row['congestion_coords']:\n",
        "            folium.CircleMarker(\n",
        "                location=coord,\n",
        "                radius=5,\n",
        "                color=\"red\",\n",
        "                fill=True,\n",
        "                fill_color=\"red\",\n",
        "                fill_opacity=0.8,\n",
        "                popup=f\"<b>Congestion Detected</b>\"\n",
        "            ).add_to(traffic_map)\n",
        "\n",
        "# Define a list of distinct colors\n",
        "route_colors = [\n",
        "    \"blue\", \"green\", \"black\", \"purple\", \"orange\", \"darkred\", \"darkblue\",\n",
        "    \"cadetblue\", \"darkpurple\", \"lightblue\", \"lightgreen\", \"lightgray\",\n",
        "   \"pink\", \"beige\", \"lightred\"\n",
        "]\n",
        "\n",
        "# Plot traffic routes (multiple alternatives)\n",
        "for idx, row in df.iterrows():\n",
        "    if row['polyline']:  # Ensure we have routes\n",
        "        for i, route in enumerate(row['polyline']):  # Iterate through all alternative routes\n",
        "            color = route_colors[i % len(route_colors)]  # Cycle through colors\n",
        "\n",
        "            folium.PolyLine(\n",
        "                locations=route,\n",
        "                color=color,\n",
        "                weight=3,\n",
        "                opacity=0.8,\n",
        "                tooltip=f\"Route {i + 1} from {row['origin']} to {row['destination']}\"\n",
        "            ).add_to(traffic_map)\n",
        "\n",
        "\n",
        "\n",
        "# Add traffic routes (blue)\n",
        "for _, row in df.iterrows():\n",
        "    if row['polyline']:\n",
        "        folium.PolyLine(\n",
        "            row['polyline'][0],\n",
        "            color=\"blue\",\n",
        "            weight=3,\n",
        "            opacity=0.8\n",
        "        ).add_to(traffic_map)\n",
        "\n",
        "\n",
        "\n",
        "# Plot Cameras (CCTV) - Blue Camera Icons\n",
        "for _, row in camera_df.iterrows():\n",
        "    folium.Marker(\n",
        "        location=[row[\"Latitude\"], row[\"Longitude\"]],\n",
        "        popup=f\"Camera: {row['Name']}\",\n",
        "        icon=folium.Icon(color=\"blue\", icon=\"camera\"),\n",
        "    ).add_to(traffic_map)\n",
        "\n",
        "# Plot DMS (Message Signs) - Green Info Icons\n",
        "for _, row in dms_df.iterrows():\n",
        "    folium.Marker(\n",
        "        location=[row[\"Latitude\"], row[\"Longitude\"]],\n",
        "        popup=f\"DMS: {row['Name']}\",\n",
        "        icon=folium.Icon(color=\"green\", icon=\"info-sign\"),\n",
        "    ).add_to(traffic_map)\n",
        "\n",
        "\n",
        "\n",
        "# Save the combined map\n",
        "traffic_map.save(\"traffic_map_with_routes_cameras_congestion.html\")\n",
        "print(\"Map saved as 'traffic_map_with_routes_cameras_congestion.html'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClQrJkohPen7",
        "outputId": "ea292efe-d5da-4196-c57d-a320fdf64665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Map saved as 'traffic_map_with_routes_cameras_congestion.html'\n"
          ]
        }
      ]
    }
  ]
}